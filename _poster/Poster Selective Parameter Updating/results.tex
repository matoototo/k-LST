\vspace{-0.5em}
We have evaluated the performance of our methods in terms of accuracy and F1-score on the SST-2 dataset, as well as peak memory usage and training samples per second.

\vspace{\baselineskip}
Our SiVA method is closely related to LoRA, while k-LST is a generalization of LST. We show that our methods improve upon their respective baselines. As a result, we present a set of new PEFT methods that cover both the low memory and full fine tuning quality regimes.

\begin{table}
  \caption{Comparison of Different Methods ($\text{{batch size}}= 16$)}
  \label{method-comparison}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Method           & Accuracy & F1    & Memory Usage (MB) & Samples per Second    \\
    \midrule
    Full Fine-Tuning & 96.44    & 96.52 & 8188              & 51.8                  \\
    LoRA             & 96.22    & 96.30 & 4526              & 64.6                  \\
    LST              & 93.23    & 93.33 & 1779              & 73.8                  \\
    Last 3 Layers    & 94.04    & 94.18 & 2117              & 179.6                     \\
    \midrule
    SiVA             & 96.56    & 96.63 & 4529              & 73.3                  \\
    SiVA - key value & 95.76    & 95.86 & 2920              & 106.4                 \\
    \midrule
    5-LST            & 94.04    & 94.04 & 2209              & 60.3                  \\
    9-LST            & 95.07    & 95.18 & 2381              & 52.4                  \\
    \midrule
    MeZO    &   91.74   & 91.96     &   1805    &   54.5
                    \\
    \midrule
    LST + Prompt  & 94.84    & 94.98 & 1803              & 73.8                  \\
    9-LST + Prompt& 95.41    & 95.55 & 2428              & 52.4                  \\
    Last 3 Layers + Prompt & 94.84 & 94.87 & 2122 & 179.6 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{itemize}
    \item SiVA can be used to achieve a balanced reduction in memory usage and computation time when no compromise can be made on model performance.
    \item k-LST can be used to achieve the highest reduction in memory usage time with minimal loss of model performance.
    \item Prompting can be used to further increase k-LST fine-tuning performance at no additional cost.
\end{itemize}

\vspace{\baselineskip}
