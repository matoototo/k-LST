%YAML 1.2
---
dataset:
    name: "sst2"

model:
    base_model: "t5-base"
    model_type: "t5"
    lora:
        scaling_rank: 1
        rank: 0
        init_scale: 0.00
        layers: "k|v|wi.*" # "k|v|wi_1.*" for t0
        modules: ".*SelfAttention|.*EncDecAttention|.*DenseReluDense"
        buffer_original: False

freeze:
    strategy: "none"

optimizer:
    name: "adafactor"
    lr: 0.003
    weight_decay: 0
    scale_parameter: True
    scheduler: "linear_decay_with_warmup"
    warmup_ratio: 0.06
    trainable_param_names: ".*lora_b.*"

train:
    num_train_epochs: 2
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    output_dir: "results"
    evaluation_strategy: "epoch"

...
