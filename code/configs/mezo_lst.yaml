%YAML 1.2
---
dataset:
    name: "sst2"

model:
    base_model: "roberta-large"
    model_type: "bert"
    modifier: "with_prompt"

freeze:
    strategy: "all_but_last_n"
    args:
        n: 0

adapter:
    strategy: "lst"
    args:
        reduction_factor: 8
        fusion: "attention" # "additive", "dynamic", "gated" or "attention"
        freeze_head: true
        k: 1

optimizer:
    name: "adamw"
    lr: 0.00002
    weight_decay: 0.01
    scale_parameter: True
    scheduler: "linear_decay_with_warmup"
    warmup_ratio: 0.06

train:
    num_train_epochs: 10
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    output_dir: "results"
    evaluation_strategy: "steps"
    eval_steps: 500
    save_strategy: "steps"
    logging_steps: 100
    save_steps: 1000
    save_total_limit: 1
...
